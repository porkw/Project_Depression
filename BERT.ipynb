{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu-gpu/anaconda3/envs/trainEnv/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\" #英文pretrain(不區分大小寫)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict size 21128\n",
      "token               index          \n",
      "-------------------------\n",
      "##tics              11130\n",
      "cream               11494\n",
      "##皱                 17708\n",
      "##喫                 14661\n",
      "url                  8654\n",
      "##ｐ                 10748\n",
      "##趕                 19691\n",
      "##息                 15679\n",
      "##ナー                12005\n",
      "隴                    7404\n"
     ]
    }
   ],
   "source": [
    "# get pre-train tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "vocab = tokenizer.vocab\n",
    "print(\"dict size\", len(vocab))\n",
    "\n",
    "# see some token and index mapping\n",
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids): #隨便看幾個字\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "val = pd.read_csv('validation.csv')\n",
    "total = pd.concat([train, val])\n",
    "total = total.reset_index(drop=True)\n",
    "total_0 = total[total[\"Label\"] == 0]\n",
    "total_1 = total[total[\"Label\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label_1\n",
    "num_1 = total_1.shape[0]\n",
    "random_1 = random.sample(range(num_1), num_1)\n",
    "train_1 = total_1.iloc[random_1[:108]]\n",
    "test_1 = total_1.iloc[random_1[108:135]]\n",
    "# Label_0\n",
    "num_0 = total_0.shape[0]\n",
    "random_0 = random.sample(range(num_0), num_0)\n",
    "train_0 = total_0.iloc[random_0[:400]]\n",
    "test_0 = total_0.iloc[random_0[400:500]]\n",
    "# train_val_1\n",
    "train_val_1 = train_1.shape[0]\n",
    "random_train_val = random.sample(range(train_val_1), train_val_1)\n",
    "train__1 = train_1.iloc[random_train_val[:97]]\n",
    "val_1 = train_1.iloc[random_train_val[97:108]]\n",
    "# train_val_0\n",
    "train_val_0 = train_0.shape[0]\n",
    "random_train_val = random.sample(range(train_val_0), train_val_0)\n",
    "train__0 = train_0.iloc[random_train_val[:360]]\n",
    "val_0 = train_0.iloc[random_train_val[360:400]]\n",
    "# train_val\n",
    "train = pd.concat([train__1, train__0])\n",
    "train = train.reset_index(drop=True)\n",
    "val = pd.concat([val_1, val_0])\n",
    "val = val.reset_index(drop=True)\n",
    "# test\n",
    "test = pd.concat([test_1, test_0])\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剔除過長的樣本以避免 BERT 無法將整個輸入序列放入記憶體不多的 GPU\n",
    "MAX_LENGTH = 250\n",
    "train = train[~(train.Content.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "test = test[~(test.Content.apply(lambda x : len(x)) > MAX_LENGTH)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_a = train['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      每天都很空洞不知道自己到底為什麼下班之後更是常常莫名想哭莫名的不想動莫名的一直自己掐住喉嚨到...\n",
       "2      一直撐著不對自己下手但又一直痛苦著好痛好痛好難過我已經不行了遺書一直寫了又刪寫了又刪我撐不下...\n",
       "3      憂鬱症到現在也大概兩年多了第一次吃藥因為藥太重出現混亂的狀態很快就被要求休學回老家休息後來還...\n",
       "4      一下班強迫症跟憂鬱又大發作明明只是第二天上班步調好快我跟不上根本也都沒準時下班又沾東沾西要死...\n",
       "5      系館旁有一小片樹林，我們稱之為黑森林。黑森林並不黑，隔壁是個水池，裡面有烏龜在游泳。我跟學妹...\n",
       "                             ...                        \n",
       "449    好痛苦， 每天從晚上12點睡到晚上6點， 睡十幾個小時， 醒來沒多久又要睡了。 做不了什麼事...\n",
       "453    https://i.imgur.com/lJznqj4.jpg心理狀態越來越糟也許下次回診該...\n",
       "454    年輕時就已經一直失業了現在都快步了中年了，年紀愈來愈大，失業時工作愈難找明明知道不能挑工作，...\n",
       "455    四月，我因為自殺未遂進了醫院。五月，六月，我都在精神科病房度過。精神科病房規定不能攜帶任何電...\n",
       "456    哈囉大家好我是精靈本期我將會介紹世界上數一數二知名的遊戲公司—MOJANG就是開發MINEC...\n",
       "Name: Content, Length: 294, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(text):\n",
    "    # 建立第一個句子的 BERT tokens\n",
    "    for i in range(len(text)):\n",
    "        tokens_a = tokenizer.tokenize(text[i])\n",
    "        len_a = len(word_pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "token(text_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 1333, 3315,  100, 2769,  100,  100, 2769,  100,  677, 4638,  100,\n",
       "         100, 2769,  100,  100,  100, 3315,  100,  100,  100,  100,  100,  100,\n",
       "         100, 2769, 4638,  100, 7279,  100,  100,  100,  677,  100,  722, 2527,\n",
       "         100,  100,  679,  100,  100,  100,  671,  100,  752,  100,  100, 2769,\n",
       "         100, 4638,  100,  100,  100,  753, 4638, 5739, 3152, 1366,  100,  100,\n",
       "         100,  100,  100,  100,  677, 1398,  100, 6963,  100,  671,  100,  100,\n",
       "        2695,  100, 1921, 4638,  100,  100,  100,  100,  677,  100, 6963,  100,\n",
       "         100,  100,  100,  100,  100,  679,  100, 4638,  738,  100,  100, 2769,\n",
       "         100,  100,  100, 2399,  100,  782, 6963,  100,  100,  100,  100,  100,\n",
       "         100, 1922,  100,  100,  100, 2769,  671,  100,  100,  100,  100,  100,\n",
       "         100,  100,  100,  100,  100,  100,  100,  100,  671,  100,  100,  100,\n",
       "        4638, 5739, 3152, 2527,  100, 2769,  100,  100,  100,  100,  100,  100,\n",
       "         100,  100,  752,  100,  100,  100,  100,  100,  100, 3300,  100, 4638,\n",
       "         782, 8020,  100,  100,  100, 2207, 8021,  100, 2527, 2769,  738,  100,\n",
       "         100,  100,  100,  100,  100,  100,  100, 3300,  100, 2552,  100,  100,\n",
       "         100,  100,  100,  100,  100, 4638,  100,  100,  100,  100,  100,  100,\n",
       "        5739, 3152,  677,  100,  100, 5739, 3152,  100,  100,  100, 4495,  679,\n",
       "         100, 4638,  100,  100,  100, 2527,  100,  100,  704, 3152,  511,  100,\n",
       "         100,  100,  123,  100,  100,  100,  100,  100,  100,  100,  100, 2769,\n",
       "         100,  100,  100,  118,  118,  102])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將整個 token 序列轉換成索引序列\n",
    "ids = tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "tokens_tensor = torch.tensor(ids)\n",
    "tokens_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trainEnv",
   "language": "python",
   "name": "trainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
